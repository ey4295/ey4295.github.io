---
layout:     post
title:      CS224学习笔记补充-矩阵求导
subtitle:   矩阵求导
date:       2018-05-06
author:     王难
header-img: img/post-bg-ios9-web.jpg
catalog: 	 true
tags:
    - CS224
    - 矩阵求导
---
# 1.Introduction


&emsp;线性代数在机器学习以及深度学习中扮演着重要的角色,涉及到向量的运算,矩阵的运算,特征值特征向量的求解等.在这里对于这些基础知识不再赘述,本文的重点在于矩阵的求导.本文主要参考吴恩达CS229课程的补充材料.

# 2.Gradient


&emsp;假设存在函数 f : R m×n → R 以一个m×n的矩阵作为输入,以**实数值**作为函数的输出,那么这个函数的关于矩阵A的梯度可以表示为
![image.png](https://upload-images.jianshu.io/upload_images/12011882-8d2343f5b8e4168a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;也可以表示成下面这种形式
![image.png](https://upload-images.jianshu.io/upload_images/12011882-bc46f37808740773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;可以很容易看出,梯度的维度是和被求导矩阵的维度是一致的.因此,针对向量的特殊情况,就有如下表达式:
![image.png](https://upload-images.jianshu.io/upload_images/12011882-c589da0da797581d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;需要注意的一点是,上述性质只试用于函数是**实值函数**的情况,如果函数的值是向量或者矩阵,则不存在上述的性质.
&emsp;此外,矩阵的梯度仍然具有下列两个性质:
![image.png](https://upload-images.jianshu.io/upload_images/12011882-3c2c4c87ff432d59.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
------------
&emsp;现在,我们需要讨论一个在出现矩阵梯度时不可避免的问题:符号的歧义.理论上讲,梯度只是多元函数在求偏导数的一个很自然的扩展应用.然而,在实际操作时我们经常会遇到符号歧义的情况.举例来说,矩阵A ∈ R<sup> m×n</sup> 是一个固定系数的矩阵,向量b∈ R<sup>m</sup>是一个固定系数的向量,函数 f : R<sup>m</sup> → R 是一个输入为m维向量的实值函数.f的具体定义为: f (z) = z<sup>T</sup>z, 所以对应的梯度是∇ <sub>z</sub> f (z) = 2z.至此都没有什么问题,但是考虑如下表达式:
![image.png](https://upload-images.jianshu.io/upload_images/12011882-1c5887e00d39ed39.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;我们该怎样理解这样一个表达式,最起码可以有两种理解方式:
&emsp;1.参考前文所写,输入是向量,结果是实值,梯度是∇ <sub>z</sub> f (z) = 2z,则有
![image.png](https://upload-images.jianshu.io/upload_images/12011882-7aca753389e2a171.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;2.如果把这个函数看做是x的函数,那么梯度求导就应是针对x的求导,A应该看作是常数,梯度结果就应该是
![image.png](https://upload-images.jianshu.io/upload_images/12011882-6064e98f15bb0f9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;我们能够看出,问题主要在于针对谁在求梯度,所以在表示梯度的时候最好使用下表对梯度的求导对象进行标注.

# 3.Hessian


假设函数 f : R<sup>n</sup> → R 是输入为n维的向量,输出为实值的函数,那么关于向量x的Hessian矩阵可以写作,∇ <sub>2</sub>x f (x) 或者简单的表示为H,具体的形式如下
matrix of partial derivatives,
![image.png](https://upload-images.jianshu.io/upload_images/12011882-33fb65f86e7e0471.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;Hessian可以看做是二阶导,Gradient可以看做是一阶导数.

# 4.Useful deductions

#### 1.Linear Algebra Properties
![image.png](https://upload-images.jianshu.io/upload_images/12011882-680ee8c642d692e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-83e5e994ffe73e9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
#### 2.Matrix Derivatives
![](https://upload-images.jianshu.io/upload_images/12011882-9a314f61a476effc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-523772e091213e21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-3660b9b0ecdd639c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-783efd6ead000e20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-012b56144452669d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-97be4b9d78c1bf9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-10deaa8d602b5917.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
# 5 矩阵求导术
&emsp;&emsp;本部分主要参考[这篇博客](https://zhuanlan.zhihu.com/p/24709748),写的可以说是非常棒啦.核心思想是下面这个公式
![image.png](https://upload-images.jianshu.io/upload_images/12011882-5b7eb4ce78c0e71b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;所有的矩阵求导的计算都可以利用这样一个公式和微分建立起联系,这个公式直观上也是很好理解的,即全微分等于偏微分之和,然后使用矩阵的迹的方法改写形式(迹具有很多可以利用的属性,改写成这种形式有助于后面的计算).包括连式法则,也都是凑出这个形式之后可以对应的写出偏导数的形式.
&emsp;&emsp;下面给出一些常用的公式:
&emsp;&emsp;1.加减法
![image.png](https://upload-images.jianshu.io/upload_images/12011882-c050dcf37702fbf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;2.乘法
![image.png](https://upload-images.jianshu.io/upload_images/12011882-bec59f4dade196db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;3.转置
![image.png](https://upload-images.jianshu.io/upload_images/12011882-f2bf898132b86687.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;4.迹
![image.png](https://upload-images.jianshu.io/upload_images/12011882-a1eda944449f2c7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;5.逆
![image.png](https://upload-images.jianshu.io/upload_images/12011882-a59c7e90fda701ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;6.行列式(第二个公式仅可逆时成立)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-3952985d6c1c85e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-de62faa124ecd89d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;7.逐元素相乘
![image.png](https://upload-images.jianshu.io/upload_images/12011882-7f466fa60d32c357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;8.逐元素函数
![image.png](https://upload-images.jianshu.io/upload_images/12011882-26c62e1386d74e49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;9.迹套给常数
![image.png](https://upload-images.jianshu.io/upload_images/12011882-ae4d8aecf72a51df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;10.转置
![image.png](https://upload-images.jianshu.io/upload_images/12011882-b6955bedaff69dc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&emsp;&emsp;11.线性
![image.png](https://upload-images.jianshu.io/upload_images/12011882-336da7b47423e85c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&emsp;&emsp;12.乘法交换
![image.png](https://upload-images.jianshu.io/upload_images/12011882-0c413fe3f483379d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&emsp;&emsp;13.矩阵乘法/逐元素乘法交换
![image.png](https://upload-images.jianshu.io/upload_images/12011882-774a45d62488ff11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 6 examples
![image.png](https://upload-images.jianshu.io/upload_images/12011882-45fb52d629ae0c55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-ce039ecd1749972b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-ef5ca7c67757a251.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;看到这里需要多说两句,本文的存在就是因为在计算softmax权重矩阵时遇到了障碍,才对矩阵求导进行了更为深入的理解;甚至在学习了这种方法之后还是没能够给出正确的答案.错误点在于,我的解法是严格按照连式法则,一步一步的求导;而答案的解法则是先带入中间表达式,化简形式之后再求导;这本来是一样的,只不过是计算难度上的差距.却在计算的过程中给我造成了很大的困扰.关键的的错误点在于,softmax函数并不是一个逐元素函数(仔细看分母),不能应用逐元素函数求微分的法则.
![image.png](https://upload-images.jianshu.io/upload_images/12011882-9486e88bac176547.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image.png](https://upload-images.jianshu.io/upload_images/12011882-750bfeb83335cf47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


